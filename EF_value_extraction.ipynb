{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writeup\n",
    "100% LVEF extraction accuracy against human labeled data on a random sample of 100 echo notes using several rules:\n",
    "1. preprocess and tokenize text\n",
    "1. search for matches of ['LVEF', 'EF', 'ejection fraction'] in the cleaned tokens\n",
    "1. for each match, take the two tokens preceeding the match and the three words proceeeding the match\n",
    "1. return the first token containing a numeric character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "value_descriptions = ['LVEF', 'EF', 'ejection fraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mimiciii_noteevents_random_100_annotated.csv')\n",
    "df.ef_value.replace('None', np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Preprocess text for tokenization\"\"\"\n",
    "    # standardize symbols\n",
    "    text_to_symbols = {'percent': '%', 'equals': '='}\n",
    "    for symbol_word, symbol in text_to_symbols.items():\n",
    "        text = text.replace(symbol_word, symbol)\n",
    "\n",
    "    # add whitespace before junk tokens to make sure they aren't included with extracted number\n",
    "    equality_signs = ['<', '>', '=']\n",
    "    for s in equality_signs:\n",
    "        text = text.replace(s, ' ' + s)\n",
    "\n",
    "    # add whitespace before junk tokens to make sure they aren't included with extracted number\n",
    "    junk = [',', '.']\n",
    "    for s in junk:\n",
    "        text = text.replace(s, ' ' + s)\n",
    "\n",
    "    # keep % sign with preceeding number\n",
    "    precede_pct = [' ', '\\n']\n",
    "    for s in precede_pct:\n",
    "        text = text.replace(s + '%', '%')\n",
    "\n",
    "    # remove parentesise\n",
    "    remove_strs = ['(', ')', '[', ']', '{', '}']\n",
    "    for s in remove_strs:\n",
    "        text = text.replace(s, '')\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_aoi(tokens, value_descriptions):\n",
    "    \"\"\"Get areas of interest (lists of tokens) for descripions of tests `value_descriptions`.\"\"\"\n",
    "    aois = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        for value_description in value_descriptions:\n",
    "            vd_tokens = tokenizer.tokenize(value_description)\n",
    "            if all(vd_token == tokens[i + j] for j, vd_token in enumerate(vd_tokens)):\n",
    "                aois.append(tokens[i - 2: i + len(vd_tokens) + 3])\n",
    "    return aois\n",
    "\n",
    "def contains_num(s):\n",
    "    \"\"\"Return True if string `s` contains a number\"\"\"\n",
    "    for c in s:\n",
    "        if c.isnumeric():\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def aois_to_value(aois):\n",
    "    \"\"\"Extract the numeric value from aoi list (list of tokens) `aois`.\"\"\"\n",
    "    for aoi in aois:\n",
    "        for token in aoi:\n",
    "            # return the first token with a numeric character in it\n",
    "            if contains_num(token):\n",
    "                return token\n",
    "    return np.NaN\n",
    "\n",
    "def text_to_value(text):\n",
    "    \"\"\"Extract the numeric value from `text`.\"\"\"\n",
    "    text = preprocess(text)\n",
    "    tokens = text.split()\n",
    "    aois = get_aoi(tokens, value_descriptions)\n",
    "    return aois_to_value(aois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% of extracted values match human labels\n"
     ]
    }
   ],
   "source": [
    "# summarize accuracy (('Mimatched labels)\n",
    "df['extracted_lvef'] = df.text.map(text_to_value)\n",
    "df['extraction_matches_label'] = df.ef_value.fillna('') == df.extracted_lvef.fillna('')\n",
    "print('{:.0f}% of extracted values match human labels'.format(\n",
    "    df.extraction_matches_label.value_counts(normalize=True)[True] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mimatched labels (extracted vs human label)\n"
     ]
    }
   ],
   "source": [
    "# identify mismatches\n",
    "mismatches = df[['ef_value', 'extracted_lvef', 'text', 'row_id']][~df.extraction_matches_label]\n",
    "print('Mimatched labels (extracted vs human label)')\n",
    "for idx, mismatch in mismatches.iterrows():\n",
    "    print('[{}] extracted value {}, human label {}'\n",
    "          '\\n-----------------------------------------\\n\\n{}'\n",
    "          '\\n\\n----------------------------------------\\n'.format(\n",
    "        mismatch.row_id, mismatch.extracted_lvef, mismatch.ef_value, mismatch.text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
